\documentclass[10pt]{scrartcl}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}

\usepackage{multicol}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{cclicenses}
\usepackage{textcomp}

\setkomafont{section}{}
\setkomafont{subsection}{}
\setkomafont{subsubsection}{}
\setkomafont{paragraph}{}
\setkomafont{subparagraph}{\normalfont\itshape}

\usepackage[landscape,paper=a4paper,left=2mm,right=2mm,top=2mm,bottom=4mm,footskip=2mm]{geometry}
\usepackage[extreme]{savetrees}

%opening
\title{Data Mining (FS 2016)}
\author{Tim Taubner}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Sim}{Sim}
\DeclareMathOperator{\Proj}{Proj}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand{\eps}{\epsilon}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\inuar}{\in_{\text{u.a.r.}}}

\setlength{\columnsep}{2mm}

\begin{document}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyfoot[L]{\footnotesize Data Mining Summary for exam use. HS 2016, \textcopyright Tim Taubner. Creative Commons 2.0 \byncsa}
\fancyfoot[R]{\footnotesize \thepage}

\pagenumbering{roman}
\begin{centering}
\vspace*{1em}
\vfill
\textbf{Disclaimer} \\
I wrote this to my best knowledge, however, no guarantees are given whatsoever.
\vfill
\textbf{Sources} \\
If not noted differently, the source is the lecture slides and/or the accompanying book.
\vfill
\end{centering}

\pagenumbering{arabic}
\setcounter{page}{0}

\clearpage

\begin{multicols}{3}

\section{Approximate Retrieval}
\paragraph{Nearest-Neighbor} Find $x^* = \argmin_{x \in X} \ d(x,y)$
given $S,\ y \in S,\ X\subseteq S$.
\paragraph{Near-Duplicate detection}
Find all $\bm x, \bm x'\in X$ with $d(\bm x,\bm x') \leq \eps$.
\subsection{$k$-Shingling}
Documents (or videos) as set of $k$-shingles (a.\ k.\ a.\ $k$-grams).
\emph{$k$-shingle} is consecutive appearance of $k$ chars/words. \\
Binary \emph{shingle matrix} $M \in \{0,1\}^{CxN}$ where $M_{i,j} = 1$ iff document $j$ contains shingle $i$, $N$ documents, $C$ $k$-shingles.
\subsection{Distance functions}
\paragraph{Def.}
$d: S \times S \rightarrow \R$ is \emph{distance function} iff pos.\ definite except $d(x,x) = 0$ ($d(x,x') > 0 \iff x \neq x'$), symmetric ($d(x,x') = d(x',x)$) and triangle inequality holds ($d(x,x'') \leq d(x,x') + d(x',x'')$).
\paragraph{$L_r$-norm}
$d_r(x,y) = ||x-y||_r = (\sum_i |x_i - y_i|^r)^{1/r}$. $L_2$ is \emph{Euclidean}.
\paragraph{Cosine}
$\Sim_c(A,B) = \dfrac{A \cdot B}{||A||\cdot||B||},\ d_c(A,B) = \dfrac{\arccos(\Sim_c(A,B))}{\pi}.$
\paragraph{Jaccard sim., d.}
$\Sim_J(A,B) = \dfrac{|A \cap B|}{|A \cup B|},\ d_J(A,B) = 1 - \Sim_J(A,B).$
% Potentially edit distance and/or hamming distance

\subsection{LSH -- local sensitive hashing}
\emph{Key Idea:} Similiar documents have similiar hash. \\
\emph{Note:} Trivial for exact duplicates (hash-collision $\rightarrow$ candidate pair).
\paragraph{Min-hash $h_\pi(C)$}
Hash is the \emph{min (i.e.\ first) non-zero permutated row index}: $h_\pi(C) = \min_{i, C(i) = 1} \pi(i)$,
bin.\ vec.\ $C$, rand.\ perm.\ $\pi$. \\
\emph{Note:} $\Pr_\pi[h_\pi(C_1) = h_\pi(C_2)] = \Sim_J(C_1,C_2)$ if $\pi \inuar S_{|C|}$.
\paragraph{Min-hash signature matrix $M_S \in [N]^{n\times C}$}
with $M_S(i,c) = h_i(C_c)$
given $n$ hash-fns $h_i$ drawn randomly from a universal hash family.

\paragraph{Pseudo permutation}
$h_\pi$ with $\pi(i) = (a\cdot i + b) \mod\ p \mod\ N$, $N$ number of shingles, $p\geq N$ prime and $a,b \inuar [p]$ with $a \neq 0$. \\
Use as universal hash family. Only store $a$ and $b$. Much more efficient.
% hash family for cosine: sign(w^Tv), from ex. 1

\paragraph{Compute Min-hash signature matix $M_S$}
For column $c \in [C]$, row $r \in [N]$ with $C_c(r) = 1$, $M_S(i,c) \leftarrow \min\{h_i(C_c), M_S(i,c)\}$ for all $h_i$.

\paragraph{$(d_1,d_2,p_1,p_2)$-sensitivity} of a hash family $F = \{h_1,\dots,h_n\}$: $\forall x,y \in S: d(x,y) \leq d_1 \implies P[h(x)=h(y)] \geq p_1$ and $d(x,y) \geq d_2 \implies P[h(x)=h(y)] \leq p_2.$
\paragraph{$r$-way AND}
$h = [h_1,\dots,h_r],\ h(x) = h(y) \Leftrightarrow \forall i\ h_i(x) = h_i(y)$ is $(d_1,d_2,p_1^r,p_2^r)$-sensitive.
\paragraph{$b$-way OR}
$h = [h_1,\dots,h_b],\ h(x) = h(y) \Leftrightarrow \exists i\ h_i(x) = h_i(y)$ is $(d_1,d_2,1-(1-p_1)^b,1-(1-p_2)^b)$-sensitive.

\paragraph{Banding as boosting}
Reduce FP/FN by $b$-way OR after $r$-way AND.
Group sig.\ matrix into $b$ bands of $r$ rows.
CP match in at least one band (check by hashing).
Result is $(d_1,d_2,1-(1-p_1^r)^b,1-(1-p_2^r)^b)$-sensitive.

\paragraph{Tradeoff FP/FN}
Favor FP (work) over FN (wrong).
Filter FP by checking signature matrix, shingles or even whole documents.

% TODO: Tradeoff-formulae

\section{Supervised Learning}
\paragraph{Linear classifier} $y_i = \sign(\bm w^T\bm x_i)$ assuming $\bm w$ goes through origin.
\paragraph{Homogeneous transform} $\tilde{\bm x} = [\bm x, 1]; \tilde{\bm w} = [\bm w, b]$, now $\bm w$ passes origin.
\paragraph{Kernel}
$k$ is inner product in high-dim.\ lin.\ space: $k(\bm x,\bm y) = \langle\phi(\bm x),\phi(\bm y)\rangle$.\\
%\emph{Kernel/Gram-matrix} .\\
\emph{shift-invariance} $k(\bm x,\bm y) = k(\bm x - \bm y)$. \\
\emph{Gaussian} $k(\bm x - \bm y) = \exp(-||\bm x - \bm y||_2^2/h^2)$.
% TODO: More about kernels

\paragraph{Convex function} $f: S \rightarrow \R$ is convex iff $\forall x,x'\in S, \lambda \in [0,1], \lambda f(x) + (1-\lambda)f(x') \geq f(\lambda x + (1-\lambda)x')$, i.\ e.\ every segment lies above function. Equiv.\ bounded by linear fn.\ at every point.
%\subparagraph{Concave} $f$ concave, iff $-f$ convex.
\subparagraph{$H$-strongly convex} $f$ $H$-strongly convex iff $f(x') \geq f(x) + \nabla f(x)^T(x'-x)+\frac{H}{2}||x'-x|_2^2$, i.\ e.\ bounded by quadratic fn (at every point).
% Bonus: Lagrangian
\subsection{Support vector machine (SVM)}
\paragraph{SVM primal}
\subparagraph{Quadratic} $\min_{\bm w} \bm w^T \bm w + C\sum_{i}\xi_i$,
s.t.\ $\forall i: y_i\bm w^T \bm x_i \geq 1 - \xi_i$, slack $C$.
\subparagraph{Hinge loss}
%$\min_{\bm w} \bm w^T \bm w + C\sum_{i}\max(0,1-y_i\bm w^T\bm x_i)$,\newline
%Also written
$\min_{\bm w} \lambda \bm w^T \bm w + \sum_{i}\max(0,1-y_i\bm w^T \bm x_i)$ with $\lambda = \frac{1}{C}$.
%$\min_{\bm w} \lambda \bm w^T \bm w + \sum_{i}l(\bm w; \bm x_i, y_i)$ with $\lambda = \frac{1}{C}$.
%With \emph{hinge loss} $l(\bm w; \bm x_i, y_i) = \max(0,1-y_i\bm w^T \bm x_i)$.
\subparagraph{Norm-constrained}
$\min_{\bm w} \sum_{i}\max(0,1-y_i\bm w^T\bm x_i)$ s.t.\ $||\bm w||_2 \leq \frac{1}{\sqrt \lambda}$.
\paragraph{Lagrangian dual}
$\max_{\bm \alpha} \sum_{i}\alpha_i + \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j \bm x_i^T\bm x_j,\ \alpha_i \in [0,C]$.
Apply kernel trick: 
$\max_{\bm \alpha} \sum_{i}\alpha_i + \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j k(\bm x_i, \bm x_j),\ \alpha_i \in [0,C]$, prediction becomes $y = \sign(\sum_{i=1}^{n}\alpha_i y_i k(\bm x_i,\bm x))$.

\subsection{Convex Programming}
\paragraph{Convex program} $\min_{\bm x} f(\bm x)$, s.\ t.\ $\bm x \in S$, $f$ convex.
\paragraph{Online convex program (OCP)} $\min_{\bm w} \sum_{t=1}^{T} f_t(\bm w)$, s.\ t.\ $\bm w \in S$.
\paragraph{General regularized form} $\min_{\bm w} \sum_{i=1}^{n} l(\bm w; \bm x_i, y_i) + \lambda R(\bm w)$, where $l$ is a (convex) loss function and $R$ is the (convex) regularizer.
\paragraph{General norm-constrained form} $\min_{\bm w} \sum_{i=1}^{n} l(\bm w; \bm x_i, y_i)$, s.\ t.\ $\bm w \in S_\lambda$, $l$ is loss and $S_\lambda$ some (norm-)constraint. Note: This is an OCP.

\paragraph{Solving OCP}
\emph{Feasible set} $S \subseteq \R^d$ and \emph{start pt.} $\bm w_0 \in S$, OCP (as above).
Round $t \in [T]$: pick feasible pt.\ $\bm w_t$, get convex fn.\ $f_t$, incur $l_t = f_t(\bm w_t)$.
Regret $R_T = (\sum_{t=1}^{T} l_t) - \min_{\bm w \in S}\sum_{t=1}^{T}f_t(\bm w)$.

\paragraph{Online SVM}
$||\bm w||_2 \leq \frac{1}{\lambda}$ (norm-constr.). For new pt.\ $\bm x_t$ classify $y_t = \sign(\bm w_t^T \bm x_t)$,
incur $l_t = \max(0,1-y_t \bm w_t^T \bm x_t)$, update $\bm w_t$ (see later).
Best $L^* = \min_{\bm w}\sum_{t=1}^{T}\max(0,1-y_t \bm w^T \bm x_t)$, regret $R_t = \sum_{t=1}^{T} l_t - L^*$.

\paragraph{Online proj.\ gradient descent (OPGD)}
Update for online SVM: \\
$\bm w_{t+1} = \Proj_S(\bm w_t-\eta_t\nabla f_t(\bm w_t))$ with $\Proj_S(\bm w) = \argmin_{w'\in S} ||\bm w'-\bm w||_2$, gives regret bound $\frac{R_T}{T}\leq\frac{1}{\sqrt{T}}(||\bm w_0-\bm w^*||_2^2+||\nabla f||_2^2)$. \\
For $H$-strongly convex fn\. set $\eta_t = \frac{1}{Ht}$ gives $R_t \leq \frac{||\nabla f||^2}{2H}(1+\log T)$.

\paragraph{Stochastic PGD (SGD)}
Online-to-batch. Compute $\tilde{\bm w} = \frac{1}{T}\sum_{t=1}^{T} \bm w_t$.
If data i.\ i.\ d.: exp.\ \emph{error (risk)} $\E[L(\tilde{\bm w})] \leq L(\bm w^*) + R_T/T$, $L(\bm w^*)$ is best error (risk) possible.

\paragraph{PEGASOS}
OPGD w/ mini-batches on strongly convex SVM form. \\
$\min_w \sum_{t=1}^{T} g_t(\bm w)$, s.t. $||\bm w||_2 \leq \frac{1}{\sqrt t}$, $g_t(\bm w) = \frac{\lambda}{2}||\bm w||_2^2 + f_t(\bm w)$. \\
$g_t$ is $\lambda$-strongly convex, $\nabla g_t(\bm w) = \lambda \bm w + \nabla f_t(\bm w)$. \\
\emph{Performance} $\eps$-accurate sol.\ with prob.\ $\geq 1-\delta$ in runtime $O^*(\frac{d\cdot\log\frac{1}{\delta}}{\lambda\eps}).$

\paragraph{ADAGrad}
Adapt to geometry.
\emph{Mahalanobis norm} $||\bm w||_{\bm G} = ||\bm G\bm w||_2$. \\
$\bm w_{t+1}=\argmin_{\bm w\in S}||\bm w - (\bm w_t - \eta \bm G_t^{-1}\nabla f_t(\bm w))||_{\bm G_t}$.
Min. regret with $G_t = (\sum_{\tau = 1}^{t} \nabla f_\tau(\bm w_\tau)\nabla f_\tau(\bm w_\tau)^T)^{1/2}.$
Easily inv'able matrix with $G_t = \diag(\dots).$
$R_t \in O(\frac{||\bm w^*||_\infty}{\sqrt T}\sqrt d)$, even better for sparse data.
%for sparse data (non-zero feature with prob.\  even
%$O(\frac{||\bm w^*||_\inf}{\sqrt T}\max\{\log d, d^{1-c/2} \})$
% Bonus: Pseudo-code

\paragraph{ADAM}
Add `momentum' term: $\bm w_{t+1} = \bm w_t - \mu\bar g_t$, $g_t = \nabla f_t(\bm w)$, $\bar g_t = (1-\beta)g_t + \beta \bar g_{t-1}$, $\bar g_0 = 0$.
% Bonus: Pseudo-code
Helps for dense gradients.

\paragraph{Parallel SGD (PSGD)}
Randomly partition to $k$ (indep.) machines. Comp.\ $\bm w = \frac{1}{k}\sum_{i=1}^k \bm w_i$.
$\E[\text{err}] \in O(\eps(\frac{1}{k\sqrt\lambda}+1))$ if $T \in \Omega(\frac{\log \frac{k\lambda}{\eps}}{\eps\lambda})$.
Suitable for MapReduce cluster, multi.\ passes possible.

\paragraph{Hogwild!}
Shared mem., no sync., sparse data. [\dots]

\paragraph{Implicit kernel trick}
Map $x \in \R^d \rightarrow \phi(x) \in \R^D \rightarrow z(x) \in \R^m$, $d \ll D, m \ll D$.
Where $\phi(x)$ corresponds to a kernel $k(x,x') = \phi(x)^T\phi(x')$.

\paragraph{Random fourier features}
For shift-invariant kernels ($k(x, y) = k(x -y)$) \\
$p(\omega)=\frac{1}{2\pi}\int e^{-j\omega'\delta}k(\delta) \diff\Delta$ \\
$\omega_i \sim p, b_i \sim U(0, 2\pi) $\\
$\bm z(\bm x) \equiv \sqrt{2/m}[cos(\omega_1'\bm x+b_1) \dots cos(\omega_m'\bm x+b_m)]$ \\
In practice: pick random samples $S = \{\hat{x}_1 \dots \hat{x}_n\} \subseteq X$ \\
$\bm K_{SXij} = k(\hat{x_i}, x_j)$, $\bm K_{SSij} = k(\hat{x_i}, \hat{x_j})$ \\
approximate $\bm K = \bm K_{XS}\bm K_{SS}^{-1}\bm K_{SX}$.

\paragraph{Nyström features}
!TODO!

\section{Pool-based active Learning (semi-supervised)}
%\paragraph{Stream-based*} Data point arrives online, decide if label needed.
%\paragraph{Pool-based} Unlabeled data-set given, (sequentially) request labels.

\paragraph{Uncertainty sampl.}
$U_t(x) = U(x | x_{1:t-1}, y_{1:t-1})$, request $y_t$ for $x_t = \argmax_x U_t(x)$.
\textbf{SVM:} $x_t = \argmin_{x_i}|\bm w^T\bm x_i|$, i.e.\ $U_t(\bm x) = \frac{1}{|\bm w_t^T \bm x|}$.

\paragraph{Sub-linear time w/ LSH}
$|\bm w^T\bm x_i|$ small if $\angle_{\bm w,\bm x_i}$ close to $\pi$.\\
Hash hyperplane:
$h_{\bm u,\bm v}(\bm a,\bm b) = [h_{\bm u}(\bm a), h_{\bm v}(\bm b)] = [\sign(\bm u^T\bm a),\sign(\bm v^T\bm b)]$.
LSH hash family: $h_H(z) = h_{u,v}(\bm z,\bm z)$ if $z$ datapoint, $h_H(z) = h_{u,v}(\bm z,-\bm z)$ if $z$ query hyperplane.
$\Pr[h_H(\bm w) = h_H(\bm x)] = \Pr[h_{\bm u}(\bm w) = h_{\bm u}(\bm x)]\Pr[h_{\bm v}(-\bm w) = h_{\bm v}(\bm x)] = \frac{1}{4} - \frac{1}{\pi^2}(\angle_{\bm x,\bm w} - \frac{pi}{2})^2$.\\
Hash all unlabeled. Loop: Hash $\bm w$, req.\ labels for hash-coll., update.

\paragraph{Informativeness}
Metric of “information” gainable; $\neq$ uncertainty.
\paragraph{Version Space}
$\mathcal{V}(D) = \{\bm w\ |\ \forall(\bm x,y)\in D\ \sign(\bm w^T\bm x) = y)\}$

\paragraph{Relevant version space} given unlabeled pool $U = \{x'_1,\dots,x'_n\}$.
$\tilde{\mathcal{V}}(D;U) = \{h: U\rightarrow\{\pm 1\}\ |\ \exists\bm w \in \mathcal{V}(D)\ \forall x\in U\ \sign(\bm w^T \bm x) = h(\bm x)\}.$

\paragraph{Generalized binary search}
Init $D \leftarrow \{\}$.
While $|\tilde{\mathcal{V}}(D;U)| > 1$, comp.\ $v^\pm(x) = |\tilde{\mathcal{V}}(D \cup \{(x,\pm)\};U)|$, label of $\argmin_x \max\{v^-(x),v^+(x)\}$.

\paragraph{Approx. $|\mathcal{V}|$}
Margins of SVM $m^\pm(x)$ for labels $\{+,-\}, \forall x$. \emph{Max-min} $\max_x \min\{m^+(x),m^-(x))\}$ or \emph{ratio} $\max_x \min\{\frac{m^+(x)}{m^-(x)},\frac{m^-(x)}{m^+(x)}\}$.

\section{Model-based clustering -- Unsupervised learning}
\paragraph{k-means problem}
$\min_\mu L(\mu)$ with
$L(\mu) = \sum_{i=1}^{N}\min_j||\bm x_i-\mu_j||_2^2$ and \emph{cluster centers} $\mu = \mu_1,\dots,\mu_k$.
Non-convex! NP-hard in general!

\paragraph{LLoyd's}
Init $\mu^{(0)}$ (somehow).
\emph{Assign} all $\bm x_i$ to closest center $z_i \leftarrow \argmin_{j \in [k]} ||\bm x_i - \mu_j^{(t-1)}||_2^2$,
\emph{Update} to mean: $\mu_j^{(t)} \leftarrow \frac{1}{n_j}\sum_{i:z_i = j}\bm x_i$.
Always converge to \emph{local minimum}.

\paragraph{Online k-means} Init $\mu$ somehow.
For $t \in [n]$ find $z = \argmin_j||\mu_j-\bm x_t||_2$, set $\mu_c \leftarrow \mu_c + \eta_t(\bm x_t - \mu_c)$. For local optimum: $\sum_t \eta_t = \infty \wedge \sum_t \eta_t^2 < \infty$ suffices, e.g.\ $\eta_t = \frac{c}{t}$.

\paragraph{Weighted rep. $C$} $L_k(\mu;C) = \sum_{(w,\bm x)\in C}w\cdot\min_j||\mu_j - \bm x||_2^2$.

\paragraph{$(k,\eps)$-coreset} iff $\forall\mu: (1-\eps)L_k(\mu;D) \leq L_k(\mu;C) \leq (1+\eps)L_k(\mu;D)$.

\paragraph{$D^2$-sampling}
Sample prob.\ $p(x) = \frac{d(x,B)^2}{\sum_{x' \in X} d(x', B)^2}$.

% Maybe include importance sampling?
%\paragraph{Step 2: Importance sampling}
%$B_i = \{\bm x\in D: i \in \argmin\ d(\bm x,b_i)\}$ ?? what is i ??
%Sample $q(\bm x) \propto \frac{\alpha d(\bm x,B)^2}{c_\phi} + \frac{2\alpha\sum_{\bm x'\in B_i} d(\bm x',B)^2}{|B_i|c_\phi} + \frac{4|X|}{|B_i|}$, $c_\phi = \frac{1}{|X|}\sum_{x\in X}d(x,B)^2$, $\alpha = \log_2 k + 1$.
%Coreset size $O(\frac{dk^3}{\eps^2})$.

\paragraph{Merge coresets} union of $(k,\eps)$-coreset is also $(k,\eps)$-coreset.

\paragraph{Compress} a $(k,\delta)$-coreset of a $(k,\eps)$-coreset is a $(k,\eps+\delta+\eps\delta)$-coreset.

\paragraph{Coresets on streams} Bin.\ tree of merge-compress. Error $\propto$ height.

\paragraph{Mapreduce k-means} Construct $(k,\eps)$-coreset C, solve k-means (w/ many restarts) on coreset. (Repeat.) Near-optimal solution.

\section{$k$-armed bandits as recommender systems}
\paragraph{$k$-armed bandit}
$k$ arms. $T$ rounds, pick $i_t\in[k]$, sample $y_t \in P_i$. Max.\ $\sum_{t=1}^{T} y_t$.
\paragraph{Regret} $\mu_i$ mean of $P_i$, $\mu^* = \max_i \mu_i$. Regret $r_t = \mu^* - \mu_{i_t}, R_T = \sum_{t=1}^{T} r_t$.
\paragraph{$\eps$-greedy} \emph{Explore} u.a.r.\ prob. $\eps_t$, \emph{exploit} with prob.\ $1-\eps_t$): choose $\argmax_i\hat\mu_i$.
Suitable $\eps_t \in O(1/t)$ gives $R_T \in O(k\log T)$. Clearly unoptimal.
\paragraph{UCB1}
Init $\hat\mu_i \leftarrow 0$; try all arms.
Round $t\in (k+1) \dots T$: $UCB(i) \leftarrow \hat\mu_i+\sqrt{\frac{2\log t}{n_i}}$, $i_t \leftarrow \argmax_i UCB(i)$, obs.\ $y_t$. Upd.\ $n_{i_t} \leftarrow n_{i_t}+1, \hat\mu_{i_t}\leftarrow\hat\mu_{i_t}+\frac{y_t-\hat\mu_{i_t}}{n_{i_t}}$.
% Maybe perf. of UCB1 ?
\paragraph{contextual bandits} Round $t$: Obs.\ \emph{context} $\bm z_t \in\mathcal Z$; \emph{recommend} $\bm x_t \in \mathcal A_t$. Reward $y_t = f(\bm x_t,\bm z_t) + \eps_t$. $r_t = \max_{\bm x}f(\bm x, \bm z_t) - f(\bm x_t,\bm z_t)$. Often $f(\bm x,\bm z) = \bm w_{\bm x}^T\bm z$.

\paragraph{LinUCB}
Estimate $\hat{\bm w}_{i} = \argmin_{\bm w}\sum_{t=1}^{m}(y_t-\bm w^T\bm z_t) + ||\bm w||_2^2$.
Closed form: $\hat{\bm w}_{i} = M_{i}^{-1}D_{i}^Ty_{i}$,
$M_{i} = D_{i}^TD_{i} + I$,
$D_{i} = [z_1|\dots|z_m], y_{i} = (y_1|\dots|y_m)^T$.\\
Confidence: $\Pr\left [|\hat{\bm w}_{i}^T\bm z_t - \bm w_{i}^T\bm z_t| \leq \alpha\sqrt{\bm z_t^T M_{i}^{-1}\bm z_t}\right ] \geq 1-\delta$ if $\alpha = 1 + \sqrt{\ln(2/\delta)/2}$.

% Algorithm LinUCB

\paragraph{Hybrid Model}
$y_t = \bm w_i^T \bm z_t + \beta^T \phi(\bm x_i,\bm z_t) + \eps_t$
captures sep.\ and shared effects.

\paragraph{Rejection Sampling}
Evaluate bandit: For $t \in \N$ read log $(\bm x_1^{(t)},\dots,\bm x_k^{(t)},\bm z_t,a_t,y_t)$.
Pick $a'_t$ by algo. If $a'_t = a_t$ feed $y_t$ to algo., else ignore line. Stop after $T$ feedbacks.


\section{Submodularity}

\end{multicols}

\appendix

\end{document}
